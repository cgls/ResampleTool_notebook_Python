{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import xarray as xr\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Copernicus Global Land Service Resampler***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This notebook shows how to resample Copernicus Global Land Service vegetation-related products (i.e. NDVI, FAPAR...), based on PROBA-V observations, from 333m resolution to 1km using R-based packages and functions.\n",
    "\n",
    "It is intended for users who would like to continue, temporarily, their 1km time series, in near real-time, before switching to the new 333m baseline resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Step 1: Data selection or download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two approaches are preset to manage the data income; the user can define a specific file that is already present locally or, thanks to the downloader, select the specific product and date of interest. Both the approaches the definition of the output folder is needed.\n",
    "In this folder will be written the downloaded product and the resampled file.\n",
    "\n",
    "For more details on the products, please see the description and Product User Manuals documentation at https://land.copernicus.eu/global/products/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define the output folder. If empty the same folder of the notebook will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "folder = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Product is localy available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If the product is already available locally, the path must be defined as a string.\n",
    "\n",
    "<span style=\"color:red\">**NOTE!:Input file must be in netCDF (.nc) format.If needed a tiff file can be read but a modification according to the xarray documentation must be done.**</span>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Automatic procedure to download data from the Copernicus Global Land repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "To get access to the Copernicus Global Land repository user credential must be provided.  \n",
    "\n",
    "Credential can be obtained here https://land.copernicus.vgt.vito.be/PDF/portal/Application.html#Home throught the Register form (on the upper right part of the page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user = ''\n",
    "psw = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Select the product and date of interest. Wait until the download is finish.\n",
    "The selected date will be adjusted to the closest date available for the selected product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://land.copernicus.vgt.vito.be/manifest/'\n",
    "\n",
    "session = requests.Session()\n",
    "session.auth = (user, psw)\n",
    "\n",
    "manifest = session.get(url, allow_redirects=True)\n",
    "products = pd.read_html(manifest.text)[0][2:-1]['Name']\n",
    "products = products[products.str.contains('300_')].reset_index(drop=True)\n",
    "print(products)\n",
    "val = input('Please select the product from the list:')\n",
    "url = f'{url}{products[int(val)]}'\n",
    "\n",
    "manifest = session.get(url, allow_redirects=True)\n",
    "product = pd.read_html(manifest.text)[0][-2:-1]['Name'].values[0]\n",
    "purl = f'{url}{product}'\n",
    "r = session.get(purl, stream=True)\n",
    "rows = r.text.split('\\n')\n",
    "dates = pd.DataFrame()\n",
    "for line in rows[:-1]:\n",
    "    r = re.search(r\"\\d\\d\\d\\d(\\/)\\d\\d(\\/)\\d\\d\", line)\n",
    "    dates = dates.append(pd.DataFrame([line], index=[pd.to_datetime(r[0], format=\"%Y/%m/%d\")]))\n",
    "\n",
    "val = input('Please insert the date in the format YYYY/MM/DD:')\n",
    "\n",
    "dates = dates.sort_index()\n",
    "i = dates.index.searchsorted(dt.datetime.strptime(val, \"%Y/%m/%d\"))\n",
    "link = dates.iloc[i][0]\n",
    "filename = os.path.basename(link)\n",
    "if folder != '':\n",
    "    path = sys.path.join(folder, filename)\n",
    "else:\n",
    "    path = filename\n",
    "\n",
    "r = session.get(link, stream=True)\n",
    "\n",
    "total_size = int(r.headers.get('content-length', 0))\n",
    "block_size = 1024  # 1 Kibibyte\n",
    "t = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "\n",
    "with open(path, 'wb') as f:\n",
    "    for data in r.iter_content(block_size):\n",
    "        t.update(len(data))\n",
    "        f.write(data)\n",
    "t.close()\n",
    "if total_size != 0 and t.n != total_size:\n",
    "    print(\"ERROR, something went wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is readed and parameters are adjusted according to the product. No extra effort is needed to define the correct product parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(path, mask_and_scale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Automatic select of products parametes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'LAI' in ds.data_vars:\n",
    "    param = {'product': 'LAI',\n",
    "             'short_name': 'leaf_area_index',\n",
    "             'long_name': 'Leaf Area Index Resampled 1 Km',\n",
    "             'grid_mapping': 'crs',\n",
    "             'flag_meanings': 'Missing',\n",
    "             'flag_values': '255',\n",
    "             'units': '',\n",
    "             'PHYSICAL_MIN': 0,\n",
    "             'PHYSICAL_MAX': 7,\n",
    "             'DIGITAL_MAX': 210,\n",
    "             'SCALING': 1. / 30,\n",
    "             'OFFSET': 0}\n",
    "    da = ds.LAI\n",
    "elif 'FCOVER' in ds.data_vars:\n",
    "    param = {'product': 'FCOVER',\n",
    "             'short_name': 'vegetation_area_fraction',\n",
    "             'long_name': 'Fraction of green Vegetation Cover Resampled 1 Km',\n",
    "             'grid_mapping': 'crs',\n",
    "             'flag_meanings': 'Missing',\n",
    "             'flag_values': '255',\n",
    "             'units': '',\n",
    "             'valid_range': '',\n",
    "             'PHYSICAL_MIN': 0,\n",
    "             'PHYSICAL_MAX': 1.,\n",
    "             'DIGITAL_MAX': 250,\n",
    "             'SCALING': 1. / 250,\n",
    "             'OFFSET': 0}\n",
    "    da = ds.FCOVER\n",
    "elif 'FAPAR' in ds.data_vars:\n",
    "    param = {'product': 'FAPAR',\n",
    "             'short_name': 'Fraction_of_Absorbed_Photosynthetically_Active_Radiation',\n",
    "             'long_name': 'Fraction of Absorbed Photosynthetically Active Radiation Resampled 1 KM',\n",
    "             'grid_mapping': 'crs',\n",
    "             'flag_meanings': 'Missing',\n",
    "             'flag_values': '255',\n",
    "             'units': '',\n",
    "             'valid_range': '',\n",
    "             'PHYSICAL_MIN': 0,\n",
    "             'PHYSICAL_MAX': 0.94,\n",
    "             'DIGITAL_MAX': 235,\n",
    "             'SCALING': 1. / 250,\n",
    "             'OFFSET': 0}\n",
    "    da = ds.FAPAR\n",
    "elif 'NDVI' in ds.data_vars:\n",
    "    param = {'product': 'NDVI',\n",
    "             'short_name': 'Normalized_difference_vegetation_index',\n",
    "             'long_name': 'Normalized Difference Vegetation Index Resampled 1 Km',\n",
    "             'grid_mapping': 'crs',\n",
    "             'flag_meanings': 'Missing cloud snow sea background',\n",
    "             'flag_values': '[251 252 253 254 255]',\n",
    "             'units': '',\n",
    "             'PHYSICAL_MIN': -0.08,\n",
    "             'PHYSICAL_MAX': 0.92,\n",
    "             'DIGITAL_MAX': 250,\n",
    "             'SCALING': 1. / 250,\n",
    "             'OFFSET': -0.08}\n",
    "    da = ds.NDVI\n",
    "elif 'DMP' in ds.data_vars:\n",
    "    param = {'product': 'DMP',\n",
    "             'short_name': 'dry_matter_productivity',\n",
    "             'long_name': 'Dry matter productivity Resampled 1KM',\n",
    "             'grid_mapping': 'crs',\n",
    "             'flag_meanings': 'sea',\n",
    "             'flag_values': '-2',\n",
    "             'units': 'kg / ha / day',\n",
    "             'PHYSICAL_MIN': 0,\n",
    "             'PHYSICAL_MAX': 327.67,\n",
    "             'DIGITAL_MAX': 32767,\n",
    "             'SCALING': 1. / 100,\n",
    "             'OFFSET': 0}\n",
    "    da = ds.DMP\n",
    "elif 'GDMP' in ds.data_vars:\n",
    "    param = {'product': 'GDMP',\n",
    "             'short_name': 'Gross_dry_matter_productivity',\n",
    "             'long_name': 'Gross dry matter productivity Resampled 1KM',\n",
    "             'grid_mapping': 'crs',\n",
    "             'flag_meanings': 'sea',\n",
    "             'flag_values': '-2',\n",
    "             'units': 'kg / hectare / day',\n",
    "             'PHYSICAL_MIN': 0,\n",
    "             'PHYSICAL_MAX': 655.34,\n",
    "             'DIGITAL_MAX': 32767,\n",
    "             'SCALING': 1. / 50,\n",
    "             'OFFSET': 0}\n",
    "    da = ds.GDMP\n",
    "else:\n",
    "    sys.exit('GLC product not found please chek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, tail = os.path.split(path)\n",
    "pos = [pos for pos, char in enumerate(tail) if char == '_'][2]\n",
    "date = tail[pos + 1: pos + 9]\n",
    "date_h = dt.datetime.strptime(date, '%Y%m%d').date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Selection of the Area of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate users, an Area of Interest (AOI), can be defined in the format [Long_min, Long_Max, Lat_min, Lat_Max]. Values must be expressed in decimal degree.\n",
    "\n",
    "The extent will be adjusted according to the cell grid of the equivalent 1 Km product. If the AOI does not fit in the new grid, the leftish boundary will be truncated to the closest border of the 1km grid. \n",
    "\n",
    "If the AOI is not defined the entire dataset will be processed. As a global 300m data array has 120960 columns and 47040 rows, or close to 5.7bn cells, be aware that this could imply heavy use of computer resources. Xarray can easily manage this thanks to the lazy approach, but if it fails an extra effort is needed to define the chunk sizes manually. \n",
    "More info is available here: http://xarray.pydata.org/en/stable/dask.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AOI definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "AOI = [-18.58, 62.95, 51.57, 28.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "AOI extent controll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]\n",
    "def bnd_box_adj(my_ext):\n",
    "    lat_1k = np.round(np.arange(80., -60., -1. / 112), 8)\n",
    "    lon_1k = np.round(np.arange(-180., 180., 1. / 112), 8)\n",
    "    lat_300 = ds.lat.values\n",
    "    lon_300 = ds.lon.values\n",
    "    ext_1k = np.zeros(4)\n",
    "    # UPL Long 1K\n",
    "    ext_1k[0] = find_nearest(lon_1k, my_ext[0]) - 1. / 336\n",
    "    # UPL Lat 1K\n",
    "    ext_1k[1] = find_nearest(lat_1k, my_ext[1]) + 1. / 336\n",
    "    # LOWR Long 1K\n",
    "    ext_1k[2] = find_nearest(lon_1k, my_ext[2]) + 1. / 336\n",
    "    # LOWR Lat 1K\n",
    "    ext_1k[3] = find_nearest(lat_1k, my_ext[3]) - 1. / 336\n",
    "    # UPL\n",
    "    my_ext[0] = find_nearest(lon_300, ext_1k[0])\n",
    "    my_ext[1] = find_nearest(lat_300, ext_1k[1])\n",
    "    # LOWR\n",
    "    my_ext[2] = find_nearest(lon_300, ext_1k[2])\n",
    "    my_ext[3] = find_nearest(lat_300, ext_1k[3])\n",
    "    return my_ext\n",
    "if len(AOI):\n",
    "    assert AOI[0] <= AOI[2], 'min Longitude is bigger than correspond Max, ' \\\n",
    "                                   'pls change position or check values.'\n",
    "    assert AOI[1] >= AOI[3], 'min Latitude is bigger than correspond Max, ' \\\n",
    "                                   'pls change position or check values.'\n",
    "    assert ds.lon[0] <= AOI[0] <= ds.lon[-1], 'min Longitudinal value out of original dataset Max ext.'\n",
    "    assert ds.lat[-1] <= AOI[1] <= ds.lat[0], 'Max Latitudinal value out of original dataset Max ext.'\n",
    "    assert ds.lon[0] <= AOI[2] <= ds.lon[-1], 'Max Longitudinal value out of original dataset Max ext.'\n",
    "    assert ds.lat[-1] <= AOI[3] <= ds.lat[0], 'min Latitudinal value out of original dataset Max ext.'\n",
    "    adj_ext = bnd_box_adj(AOI)\n",
    "    try:\n",
    "        da = da.sel(lon=slice(adj_ext[0], adj_ext[2]), lat=slice(adj_ext[1], adj_ext[3]))\n",
    "    except Exception as ex:\n",
    "        template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "        message = template.format(type(ex).__name__, ex.args)\n",
    "        print(message)\n",
    "        raise sys.exit(1)\n",
    "else:\n",
    "    da = da.shift(lat=1, lon=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Resampling using the aggregation approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There are several approaches to resample data to a coarser resolution. \n",
    "\n",
    "As both products are provided in a regular latitude/longitude grid (plate carrée), with the ellipsoid WGS 1984 (Terrestrial radius=6378km), a Kernel can be used to achieve the resampling.\n",
    "Products, usually named with a resolution of 1 km, have a resolution of the grid equal to 1/112°. Instead, products the nominally named os 333 m are defined by a resolution of 1/336°.\n",
    "According to these values, a grid of 3x3 can be used to aggregate exactly a finner resolution product to the coarsen one.\n",
    "\n",
    "\n",
    "Most of the products, some specific values are used to define qualitative information. These need to be masked out to avoid bias in the resampling.\n",
    "\n",
    "Besides, it is advised to include a condition that at least 5 out of the 9 pixels had to have valid values (i.e. not NA). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "da_msk = da.where(da <= param['DIGITAL_MAX'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "create the coarsen dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "coarsen = da_msk.coarsen(lat=3, lon=3, coord_func=np.mean, boundary='trim', keep_attrs=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "mask the dataset according to the minumum required values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vo = xr.where(da <= param['DIGITAL_MAX'], 1, 0)\n",
    "vo_cnt = vo.coarsen(lat=3, lon=3, coord_func=np.mean, boundary='trim', keep_attrs=False).sum()\n",
    "da_r = coarsen.where(vo_cnt >= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Step 5: Check the outcome and data consolidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data will be consolidated, on the disk, in netCDF format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "da_r.name = param['product']\n",
    "da_r.attrs['short_name'] = param['short_name']\n",
    "da_r.attrs['long_name'] = param['long_name']\n",
    "prmts = dict({param['product']: {'dtype': 'f8', 'zlib': 'True', 'complevel': 4}})\n",
    "name = param['product']\n",
    "da_r.to_netcdf(rf'D:/Data/CGL_subproject_coarse_res/Tests/CGLS_{name}_1KM_R_Europe_{date}.nc', encoding=prmts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If needed dataresults can be plotted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "da_r.plot(robust=True, cmap='YlGn', figsize=(15, 10))\n",
    "plt.title(f'Copernicus Global Land\\n Resampled {name} to 1K over Europe\\n date: {date_h}')\n",
    "plt.ylabel('latitude')\n",
    "plt.xlabel('longitude')\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (HLS)",
   "language": "python",
   "name": "pycharm-11fde492"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}